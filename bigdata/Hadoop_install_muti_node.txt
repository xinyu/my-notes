操作系统: CentOS 7

主机:
master     192.168.6.104
slave01    192.168.6.112
slave02    192.168.6.107

操作系统基础安装配置:
systemctl disable NetworkManager
systemctl stop NetworkManager
chkconfig network on
systemctl restart network

yum clean all
yum repolist

yum -y update

systemctl stop chronyd.service
systemctl disable chronyd.service
yum erase -y chrony
rm -f /etc/chrony*

yum install net-tools sysstat tcpdump wget ntp ntpdate -y

sed -i -e 's/=enforcing/=disabled/g' /etc/sysconfig/selinux
sed -i -e 's/=enforcing/=disabled/g' /etc/selinux/config

systemctl stop firewalld
systemctl disable firewalld

systemctl stop postfix.service
systemctl disable postfix.service

timedatectl set-timezone Asia/Shanghai

systemctl enable ntpd
systemctl start ntpd
ntpq -p  # Verify operation

vi /etc/sysconfig/kernel   
  # MAKEDEBUG=yes
awk -F\' '$1=="menuentry " {print i++ " : " $2}' /etc/grub2.cfg
grub2-mkconfig -o /boot/grub2/grub.cfg

reboot

挂载数据盘:
mkfs.xfs -L data /dev/vdb
mkdir /data
vi /etc/fstab
LABEL=data /data xfs defaults,noatime 0 0


hadoop 环境部署: master/slave
1 安装Java JDK
cd ~
curl -O http://xxxx.xxx.xxx/jdk-8u91-linux-x64.rpm
yum -y localinstall jdk-8u91-linux-x64.rpm
java -version

2 创建用户
useradd hadoop
echo -e "hadoop\nhadoop\n" | passwd hadoop

3 下载Hadoop
wget http://mirror.rise.ph/apache/hadoop/common/hadoop-2.7.2/hadoop-2.7.2.tar.gz
wget http://mirror.rise.ph/apache/hadoop/common/hadoop-2.7.2/hadoop-2.7.2-src.tar.gz

tar zxf hadoop-2.7.2.tar.gz
mkdir /opt/hadoop
mv hadoop-2.7.2/* /opt/hadoop
chown -R hadoop:hadoop /opt/hadoop/

4 配置hadoop
hdfs master:
su - hadoop
ssh-keygen -t rsa
cd ~/.ssh/
cat id_rsa.pub >> authorized_keys
chmod 600 ./authorized_keys 
ssh localhost
logout

cat << EOF > ~/.ssh/config
Host * 
Port 22
StrictHostKeyChecking no  
UserKnownHostsFile=/dev/null 
EOF

chmod 600  ~/.ssh/config

hdfs slave X:
scp -r ~/.ssh hadoop@hdfs-X:/home/hadoop


添加环境变量，编辑.bash_profile文件：
$ vi ~/.bash_profile
在文件尾写入：
export JAVA_HOME=/usr/java/default
export PATH=$PATH:$JAVA_HOME/bin
export CLASSPATH=.:$JAVA_HOME/jre/lib:$JAVA_HOME/lib:$JAVA_HOME/lib/tools.jar
export HADOOP_HOME=/opt/hadoop
export HADOOP_INSTALL=$HADOOP_HOME
export HADOOP_MAPRED_HOME=$HADOOP_HOME
export HADOOP_COMMON_HOME=$HADOOP_HOME
export HADOOP_HDFS_HOME=$HADOOP_HOME
export YARN_HOME=$HADOOP_HOME
export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native
export PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin
使新添加的环境变量：
$ source ~/.bash_profile

编辑hadoop-env.sh文件：
$ cd $HADOOP_HOME/etc/hadoop
$ vi hadoop-env.sh
找到如下一行：
export JAVA_HOME=${JAVA_HOME}
改为：
export JAVA_HOME=/usr/java/default/

hostname:
vi /etc/hosts
192.168.6.104 hdfs-1
192.168.6.112 hdfs-2
192.168.6.107 hdfs-3

SSH免密码登陆
ssh master
ssh slave0x

配置集群环境
cd $HADOOP_HOME/etc/hadoop

vi slaves
localhost  (delete)
slave0x    (add)

vi core-site.xml
<configuration>
        <property>
                <name>fs.defaultFS</name>
                <value>hdfs://hdfs-1:9000</value>
        </property>
        <property>
                <name>hadoop.tmp.dir</name>
                <value>file:/data/hadoop/tmp</value>
                <description>Abase for other temporary directories.</description>
        </property>
</configuration>

vi hdfs-site.xml
<configuration>
        <property>
                <name>dfs.namenode.secondary.http-address</name>
                <value>hdfs-1:50090</value>
        </property>
        <property>
                <name>dfs.replication</name>
                <value>1</value>
        </property>
        <property>
                <name>dfs.namenode.name.dir</name>
                <value>file:///data/hadoop/hadoopdata/namenode</value>
        </property>
        <property>
                <name>dfs.datanode.data.dir</name>
                <value>file:///data/hadoop/hadoopdata/datanode</value>
        </property>
</configuration>

创建目录:
mkdir -p /data/hadoop
mkdir /data/hadoop/hadoopdata
mkdir /data/hadoop/hadoopdata/namenode
mkdir /data/hadoop/hadoopdata/datanode
chown -R hadoop:hadoop /data/hadoop

创建mapred-site.xml文件：
$ cp mapred-site.xml.template mapred-site.xml

vi mapred-site.xml
<configuration>
        <property>
                <name>mapreduce.framework.name</name>
                <value>yarn</value>
        </property>
        <property>
                <name>mapreduce.jobhistory.address</name>
                <value>hdfs-1:10020</value>
        </property>
        <property>
                <name>mapreduce.jobhistory.webapp.address</name>
                <value>hdfs-1:19888</value>
        </property>
</configuration>

vi yarn-site.xml
<configuration>
        <property>
                <name>yarn.resourcemanager.hostname</name>
                <value>hdfs-1</value>
        </property>
        <property>
                <name>yarn.nodemanager.aux-services</name>
                <value>mapreduce_shuffle</value>
        </property>
</configuration>

master:
初始化HDFS文件系统：
$ hdfs namenode -format

Hadoop相关的执行脚本：
启动Hadoop服务：
$ start-dfs.sh

启动YARN：
$ start-yarn.sh

mr-jobhistory-daemon.sh start historyserver

查看服务运行状态：
$ jps
$ hdfs dfsadmin -report


停止：
stop-yarn.sh
stop-dfs.sh
mr-jobhistory-daemon.sh stop historyserver


测试
默认情况下，Hadoop namenode服务使用50070端口：
http://192.168.6.104:50070

Hadoop All Applications，访问：
http://192.168.6.104:8088

查看NodeManager信息：
http://192.168.6.104:8042

cd /opt/hadoop/bin
./hadoop fs -ls /
./hadoop jar /opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar pi 16 1000
http://blog.cheyo.net/18.html

Filesystem check (fsck)
hdfs fsck /
Finding the blocks for a file
hdfs fsck /user/tom/part-00007 -files -blocks -racks

hadoop job -history output
Unable to initialize History Viewer

----------
WordCount:
hadoop-2.7.2-src/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples
export CLASSPATH=$($HADOOP_HOME/bin/hadoop classpath):$CLASSPATH
javac WordCount.java
jar -cvf WordCount.jar ./WordCount*.class

mkdir test
vi test/file00
vi test/file01

hdfs dfs -mkdir test
hdfs dfs -put ./test/file* test
hdfs dfs -ls test
$HADOOP_HOME/bin/hadoop jar WordCount.jar org.apache.hadoop.examples.WordCount test output
hdfs dfs -cat output/*
hdfs dfs -get output ./output
hdfs dfs -rm -r output

$HADOOP_HOME/bin/hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-*streaming*.jar \
-file /home/hadoop/mapper.py    -mapper /home/hadoop/mapper.py \
-file /home/hadoop/reducer.py   -reducer /home/hadoop/reducer.py \
-input test/* -output output_python